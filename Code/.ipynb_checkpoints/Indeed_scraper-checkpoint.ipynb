{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import us\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the US states\n",
    "state_names = [state.name for state in us.states.STATES_AND_TERRITORIES]\n",
    "state_names = [s.replace(' ','+') for s in state_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching state by state, storing after each state\n",
    "for state in state_names:\n",
    "    results = []\n",
    "    print('We are at state', state, '.')\n",
    "    pagecounter = 0\n",
    "    numberofemptypages = 0\n",
    "    while pagecounter <= 660:\n",
    "        print('Sleeping......')\n",
    "        time.sleep(random.randint(60,180))\n",
    "        url = \"https://www.indeed.com/jobs?l={}&start=\".format(state) + str(pagecounter)\n",
    "\n",
    "        connectionsucces = False\n",
    "        sleepamount = 300\n",
    "        while connectionsucces == False:\n",
    "            try:\n",
    "                session = requests.Session()\n",
    "                retry = Retry(connect=5, backoff_factor=0.1)\n",
    "                adapter = HTTPAdapter(max_retries=retry)\n",
    "                session.mount('http://', adapter)\n",
    "                session.mount('https://', adapter)\n",
    "                response = session.get(url)\n",
    "                connectionsucces = True\n",
    "            except:\n",
    "                print('waiting to retry connection')\n",
    "                sleepamount = sleepamount + 60\n",
    "                time.sleep(sleepamount)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title_tags = soup.find_all('h2', attrs={'class': 'title'})\n",
    "        links = [\"http://indeed.com\" + title_tag.a['href'] for title_tag in title_tags]\n",
    "        if len(links) == 0:\n",
    "            randomwait = random.randint(1800,5400)\n",
    "            print('no links are fetched, waiting', randomwait/3600, 'hours for captcha...')\n",
    "            time.sleep(randomwait)\n",
    "            numberofemptypages = numberofemptypages + 1\n",
    "        for link in links:\n",
    "            try:\n",
    "                time.sleep(random.randint(2,10))\n",
    "                res = requests.get(link)\n",
    "                #print(res.status_code, link)\n",
    "                content = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "                # getting entire job description\n",
    "                jobdescription = []\n",
    "                description = content.findAll('div', attrs={'class': 'jobsearch-jobDescriptionText'})\n",
    "                for value in description:\n",
    "                    jobdescription.append(value.text.strip())\n",
    "                try:\n",
    "                    try:\n",
    "                        results.append({\n",
    "                            'job_title': content.find('h1', attrs={\n",
    "                                'class': 'icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title'}).get_text(),\n",
    "                            'company': content.find('div', attrs={'class': 'icl-u-lg-mr--sm icl-u-xs-mr--xs'}).get_text(),\n",
    "                            'salary': content.find('span', attrs={'class': 'icl-u-xs-mr--xs'}).get_text(),\n",
    "                            'job_description': jobdescription,\n",
    "                            'url': link\n",
    "                        })\n",
    "\n",
    "                    except:\n",
    "                        try:\n",
    "                            results.append({\n",
    "                                'job_title': content.find('h1', attrs={\n",
    "                                    'class': 'icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title'}).get_text(),\n",
    "                                'company': content.find('div',\n",
    "                                                        attrs={'class': 'icl-u-lg-mr--sm icl-u-xs-mr--xs'}).get_text(),\n",
    "                                'salary': 'not_found',\n",
    "                                'job_description': jobdescription,\n",
    "                                'url': link\n",
    "                            })\n",
    "\n",
    "                        except:\n",
    "                            results.append({\n",
    "                                'job_title': content.find('h1', attrs={\n",
    "                                    'class': 'icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title'}).get_text(),\n",
    "                                'company': 'not_found',\n",
    "                                'salary': 'not_found',\n",
    "                                'job_description': jobdescription,\n",
    "                                'url': link\n",
    "                            })\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print('number of job postings', len(results))\n",
    "        pagecounter = pagecounter + 10\n",
    "\n",
    "        if numberofemptypages >= 5:\n",
    "            print('Did not find any links 5 times in a row, writing to df and moving on')\n",
    "            df = pd.DataFrame(columns=['job_title', 'company', 'salary', 'job_description', 'url'])\n",
    "            for row in results:\n",
    "                df = df.append(row, ignore_index=True)\n",
    "            df.to_csv('/home/ivo/python-scripts/indeed/indeed_jobpostings_{}.csv'.format(state), sep=';', header=True, index=True)\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(columns=['job_title', 'company', 'salary', 'job_description', 'url'])\n",
    "    for row in results:\n",
    "        df = df.append(row, ignore_index=True)\n",
    "\n",
    "    df.to_csv('/home/ivo/python-scripts/indeed/indeed_jobpostings_{}.csv'.format(state), sep=';', header=True, index=True)\n",
    "    print('Writing to df complete.')\n",
    "\n",
    "    waittime = random.randint(18000,25200)\n",
    "    print('waiting', waittime/3600, 'hours till next state')\n",
    "    time.sleep(waittime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
