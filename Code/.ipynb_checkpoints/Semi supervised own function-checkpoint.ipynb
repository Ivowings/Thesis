{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "empty-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import thesis_helper\n",
    "\n",
    "word2vecpath = \"/Users/ivowings/Sync/Thesis/Code/Word2vec models/word2vec_s100_CBOW.model\"\n",
    "\n",
    "th = thesis_helper.Thesis_Helper(word2vecpath, 'en_core_web_lg')\n",
    "\n",
    "import pandas as pd\n",
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import time\n",
    "\n",
    "seed = 456\n",
    "threshold = 0.99\n",
    "\n",
    "\n",
    "word2vec = Word2Vec.load(\"/Users/ivowings/Sync/Thesis/Code/Word2vec models/word2vec_s100_CBOW.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "brave-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to retrieve word2vec vectors from spacy\n",
    "def word2vec_retriever_sum(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens =  tokenizer.tokenize(text)\n",
    "    wordvectors = sum(word2vec.wv[tokens])\n",
    "    return wordvectors\n",
    "    \n",
    "\n",
    "def word2vec_retriever_average(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens =  tokenizer.tokenize(text)\n",
    "    wordvectors = word2vec.wv[tokens]\n",
    "    average = sum(wordvectors)/len(wordvectors)\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "every-aaron",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1056578/1056578 [00:38<00:00, 27186.85it/s]\n",
      "100%|██████████| 1056578/1056578 [02:57<00:00, 5938.83it/s] \n"
     ]
    }
   ],
   "source": [
    "path_context = \"/Users/ivowings/Sync/Thesis/Datasources/Preprocessed/n-grams context/Annotated/sentence_trigrams_lemmatized_context.csv\"\n",
    "path_trigrams = \"/Users/ivowings/Sync/Thesis/Datasources/Preprocessed/n-grams/Annotated/emscad_trigrams_lemmatized_taxonomy.csv\"\n",
    "\n",
    "\n",
    "df_context = pd.read_csv(path_context,sep=';')\n",
    "df_context['allgrams'] = df_context['left_context'].str.lower() + ' ' + df_context['recovered_gram'].str.lower() + ' ' + df_context['right_context'].str.lower()\n",
    "df_context = df_context[['allgrams', 'label']]\n",
    "\n",
    "df_trigrams = pd.read_csv(path_trigrams, sep=',')\n",
    "\n",
    "df = pd.concat([df_trigrams,df_context]).reset_index(drop=True)\n",
    "df = df.drop(columns=['length'])\n",
    "#Replacing missing labels with -1\n",
    "df['label'] = df['label'].fillna(-1).astype(int)\n",
    "\n",
    "x_vectors = pd.DataFrame(df['allgrams'].progress_apply(word2vec_retriever_average))\n",
    "x_vectors = x_vectors['allgrams'].progress_apply(pd.Series)\n",
    "\n",
    "df = x_vectors.join(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "downtown-entity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the semi supervised training method\n",
      "Number of unlabeled points  1055578\n",
      "Precision Gradient Boosting  0.5688820312107984\n",
      "Recall Gradient Boosting  0.551522180948052\n",
      "F1 score Gradient Boosting  0.5575757575757576\n",
      "After completing the run 823113 \n",
      "Iteration took  10.225259065628052  seconds \n",
      "\n",
      "\n",
      "Number of unlabeled points  823113\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Thesis/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         n_stages = self._fit_stages(\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             sample_weight_val, begin_at_stage, monitor)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Thesis/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0;31m# fit next stage of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[1;32m    562\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                 random_state, X_csc, X_csr)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Thesis/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0m\u001b[1;32m    215\u001b[0m                      check_input=False)\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Thesis/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \"\"\"\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1248\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/Thesis/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    387\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "iterate = True\n",
    "starttime = time.time()\n",
    "\n",
    "print('Starting the semi supervised training method')\n",
    "while iterate == True:\n",
    "    \n",
    "    iterationstarttime = time.time()\n",
    "    \n",
    "    #Splitting df into unlabeled, labeled and labeled into x and y\n",
    "    X_unlabeled = df[df.label==-1].drop(columns=['label'])\n",
    "    labeled_data = df[df.label>-1]\n",
    "    x = labeled_data.drop(columns=['label'])\n",
    "    y = np.ravel(labeled_data[['label']])\n",
    "    \n",
    "    print('Number of unlabeled points ',X_unlabeled.shape[0])\n",
    "\n",
    "    x_train,x_test,y_train,y_test=train_test_split(x, y, test_size=0.2,random_state=seed)\n",
    "\n",
    "    #Training classifier on labeled data\n",
    "    GBC = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "         max_depth=1, random_state=seed).fit(x_train, y_train) \n",
    "\n",
    "    #Retrieving the scores from the model\n",
    "    print('Precision Gradient Boosting ',precision_score(y_test, GBC.predict(x_test), average='macro'))\n",
    "    print(\"Recall Gradient Boosting \",recall_score(y_test, GBC.predict(x_test), average='macro'))\n",
    "    print(\"F1 score Gradient Boosting \",f1_score(y_test, GBC.predict(x_test), average='macro'))\n",
    "\n",
    "    #Retrieving the probabilities of the classes on the unlabeled dataset\n",
    "    prediction_probabilities = pd.DataFrame(GBC.predict_proba(X_unlabeled),columns=['no_skill', 'soft_skill', 'hard_skill'])\n",
    "    \n",
    "    #Matching the index in order to see where the final labels belong\n",
    "    prediction_probabilities.index = X_unlabeled.index\n",
    "\n",
    "    #Only selecting predictions which are above the threshold\n",
    "    above_threshold_predictions = pd.concat([\n",
    "                    prediction_probabilities.loc[prediction_probabilities['no_skill'] > threshold],\n",
    "                    prediction_probabilities.loc[prediction_probabilities['soft_skill'] > threshold],\n",
    "                    prediction_probabilities.loc[prediction_probabilities['hard_skill'] > threshold]\n",
    "                    ],axis=0)\n",
    "    \n",
    "    if len(above_threshold_predictions) == 0:\n",
    "        print('No predictions have been found, consider lowering the threshold')\n",
    "        iterate == False\n",
    "    \n",
    "    #Labeling the classes with 5,6,7 in order to move away from the probabilities (0-1)\n",
    "    above_threshold_predictions.no_skill[above_threshold_predictions.no_skill> threshold]  = 5\n",
    "    above_threshold_predictions.soft_skill[above_threshold_predictions.soft_skill> threshold]  = 6\n",
    "    above_threshold_predictions.hard_skill[above_threshold_predictions.hard_skill> threshold]  = 7\n",
    "\n",
    "    #Selecting the max value to remove the probabilities\n",
    "    above_threshold_predictions['label'] = above_threshold_predictions.max(axis=1)\n",
    "    #Turning the label back into the original class\n",
    "    above_threshold_predictions['label'] = above_threshold_predictions['label'].astype(int)-5\n",
    "    above_threshold_predictions = above_threshold_predictions[['label']]\n",
    "\n",
    "    #Joining the predicted labels on the existing df\n",
    "    df = df.join(above_threshold_predictions, lsuffix='_df', rsuffix='_pred')\n",
    "    df['label'] = df[[\"label_df\", \"label_pred\"]].max(axis=1).astype(int)\n",
    "    df = df.drop(columns=[\"label_df\", \"label_pred\"])\n",
    "\n",
    "    X_unlabeled = df[df.label==-1].drop(columns=['label'])\n",
    "    \n",
    "    iterationendtime = time.time()\n",
    "    \n",
    "    \n",
    "    print('After completing the run', X_unlabeled.shape[0], '')\n",
    "    print('Iteration took ',(iterationendtime- iterationstarttime)/60, ' minutes \\n\\n')\n",
    "\n",
    "endtime = time.time()\n",
    "print('Semi supervised method time ', (endtime - starttime)/60, ' minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/Users/ivowings/Desktop/semi_supervised.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-possible",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
