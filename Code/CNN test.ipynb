{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "floppy-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/saarthi-ai/sentence-classification-using-convolutional-neural-networks-ddad72c7048c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aggregate-british",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label source\n",
      "0                           Wow... Loved this place.      1   yelp\n",
      "1                                 Crust is not good.      0   yelp\n",
      "2          Not tasty and the texture was just nasty.      0   yelp\n",
      "3  Stopped by during the late May bank holiday of...      1   yelp\n",
      "4  The selection on the menu was great and so wer...      1   yelp\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path = '/Users/ivowings/Desktop/sentiment labelled sentences'\n",
    "\n",
    "filepath_dict = {'yelp': '/yelp_labelled.txt' ,\n",
    "                 'amazon': '/amazon_cells_labelled.txt',\n",
    "                 'imdb': '/imdb_labelled.txt'}\n",
    "df_list = []\n",
    "for source, filepath in filepath_dict.items():    \n",
    "    df = pd.read_csv(path+filepath, names=['sentence', 'label'], sep='\\t')\n",
    "    # Add another column filled with the source name\n",
    "    df['source'] = source \n",
    "    df_list.append(df)\n",
    "df = pd.concat(df_list)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stable-christian",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d094f9b279a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_yelp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'yelp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_yelp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split             \n",
    "from keras.preprocessing.text import Tokenizer                    \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "df_yelp = df[df['source'] == 'yelp']\n",
    "\n",
    "sentences = df_yelp['sentence'].values\n",
    "y = df_yelp['label'].values\n",
    "\n",
    "sentences_train,sentences_test,y_train,y_test = train_test_split(\n",
    "                                                sentences, y,  \n",
    "                                                test_size=0.25,  \n",
    "                                                random_state=1000)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "# Adding 1 because of  reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1                          \n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "integrated-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  \n",
    "    # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "nonprofit-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix('/Users/ivowings/Desktop/sentiment labelled sentences/glove.6B.50d.txt' ,\n",
    "                                            tokenizer.word_index,  \n",
    "                                            embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "spanish-hepatitis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1747"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "critical-customs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 2s 13ms/step - loss: 0.6913 - accuracy: 0.4819 - val_loss: 0.6752 - val_accuracy: 0.5680\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.5872 - accuracy: 0.7243 - val_loss: 0.5325 - val_accuracy: 0.7560\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.2497 - accuracy: 0.9642 - val_loss: 0.4733 - val_accuracy: 0.8120\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0541 - accuracy: 0.9912 - val_loss: 0.5420 - val_accuracy: 0.7960\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.0124 - accuracy: 0.9998 - val_loss: 0.5461 - val_accuracy: 0.8040\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.5862 - val_accuracy: 0.8080\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.6157 - val_accuracy: 0.8120\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6521 - val_accuracy: 0.8000\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6699 - val_accuracy: 0.8040\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.6892 - val_accuracy: 0.8040\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-ballet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
