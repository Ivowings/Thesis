{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-07 08:23:43 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-06-07 08:23:43 INFO: Use device: cpu\n",
      "2021-06-07 08:23:43 INFO: Loading: tokenize\n",
      "2021-06-07 08:23:43 INFO: Loading: ner\n",
      "2021-06-07 08:23:44 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from flashtext import KeywordProcessor\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import spacy\n",
    "import sys\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#for downloading spacy stuff\n",
    "#!{sys.executable} -m spacy download en_core_web_sm\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "import stanza\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "nlp2 = stanza.Pipeline(lang='en', processors='tokenize,ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities.engine import RecognizerResult, OperatorConfig\n",
    "\n",
    "# Set up the engine, loads the NLP module (spaCy model by default) and other PII recognizers\n",
    "analyzer = AnalyzerEngine()\n",
    "\n",
    "# Initialize the engine with logger.\n",
    "engine = AnonymizerEngine()\n",
    "\n",
    "def presidio_tagger(text):\n",
    "    \n",
    "    tags = analyzer.analyze(text=text,\n",
    "                       entities=[\"PHONE_NUMBER\",'EMAIL_ADDRESS','PERSON'],\n",
    "                       language='en')\n",
    "    \n",
    "    result = engine.anonymize(\n",
    "    text=text,\n",
    "    analyzer_results=tags,\n",
    "    operators={\"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"<REMOVE>\"}),\n",
    "              \"PHONE_NUMBER\": OperatorConfig(\"replace\", {\"new_value\": \"<REMOVE>\"}),\n",
    "              \"EMAIL_ADDRESS\": OperatorConfig(\"replace\", {\"new_value\": \"<REMOVE>\"})})\n",
    "    \n",
    "    return result.text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "def name_remover_stanza(text):\n",
    "    \n",
    "    try:\n",
    "        doc2 = nlp2(text)\n",
    "\n",
    "        sentences =[]\n",
    "        for sentence in doc2.sentences:\n",
    "            for token in sentence.tokens:\n",
    "                #Removing all tokens which have been tagged as PERSON\n",
    "                if 'PERSON' not in token.ner:\n",
    "                    sentences.append(token.text)\n",
    "            #Adding new line symbol after each sentence to clearly denote the end of the sentence\n",
    "            sentences.append('\\n')\n",
    "        listToStr = ' '.join([str(elem) for elem in sentences]) \n",
    "        return listToStr\n",
    "    except:\n",
    "        #Catching any errors and storing those values in errors\n",
    "        print('error_found')\n",
    "        errors.append(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"/Users/ivowings/Sync/Thesis/Datasources/Job vacancies/emscad_v1 2.csv\")\n",
    "\n",
    "# #Removing the html tags in the columns.\n",
    "# rows = []\n",
    "# for t in tqdm(df['description']):\n",
    "#     soup = BeautifulSoup(t,\"lxml\")\n",
    "#     rows.append(soup.get_text())\n",
    "# df['description'] = rows\n",
    "\n",
    "# rows = []\n",
    "# #Some columns contain nan therefore appending no text\n",
    "# for t in tqdm(df['requirements']):\n",
    "#     if str(t) == 'nan':\n",
    "#         rows.append('')\n",
    "#     else:\n",
    "#         soup = BeautifulSoup(t,\"lxml\")\n",
    "#         rows.append(soup.get_text())\n",
    "# df['requirements'] = rows\n",
    "\n",
    "# df['job_description'] = df['description'] + ' ' + df['requirements']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stanza at:  08:25:55.936629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/19 [24:34<7:22:16, 1474.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stanza at:  08:50:11.830656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 2/19 [41:57<5:45:51, 1220.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stanza at:  09:07:08.514940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 3/19 [52:46<4:15:58, 959.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stanza at:  09:18:04.609821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 4/19 [1:03:32<3:28:56, 835.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stanza at:  09:32:54.380780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▋       | 5/19 [7:00:36<32:07:22, 8260.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stanza at:  15:33:28.642450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 6/19 [15:57:46<59:15:25, 16409.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stanza at:  00:25:55.836956\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/ivowings/Sync/Thesis/Datasources/Job vacancies/emscad_v1_anonymized_complete.csv',sep=',')\n",
    "df = df.fillna('no value')\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "for column in tqdm(df.columns):\n",
    "    df[column] = df[column].str.replace('#[\\w-]+#', ' ',regex=True)\n",
    "    df[column] = df[column].str.replace('#[\\w\\s-]+#', ' ',regex=True)\n",
    "    df[column] = df[column].str.replace('^(\\+\\d{1,2}\\s)?\\(?\\d{3}\\)?[\\s.-]\\d{3}[\\s.-]\\d{4}$', ' ',regex=True)\n",
    "    df[column] = df[column].apply(presidio_tagger)\n",
    "    df[column] = df[column].str.replace('<REMOVE>', ' ',regex=False)\n",
    "    df[column] = df[column].str.replace(r'  +', ' ',regex=True)\n",
    "    print('Starting Stanza at: ',datetime.now().time())\n",
    "    df[column] = df[column].apply(name_remover_stanza)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/ivowings/Desktop/emscad_presidio.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "test = pd.read_csv('/Users/ivowings/Sync/Thesis/Datasources/Job vacancies/emscad_v1_anonymized_complete.csv',sep=',')\n",
    "\n",
    "# test = test[['job_description','fraudulent']]\n",
    "test['job_description'] = test['job_description'].str.replace('#[\\w-]+#', ' ',regex=True)\n",
    "test['job_description'] = test['job_description'].str.replace('#[\\w\\s-]+#', ' ',regex=True)\n",
    "test['job_description'] = test['job_description'].str.replace('^(\\+\\d{1,2}\\s)?\\(?\\d{3}\\)?[\\s.-]\\d{3}[\\s.-]\\d{4}$', ' ',regex=True)\n",
    "test['job_description'] = test['job_description'].progress_apply(presidio_tagger)\n",
    "# test.head()\n",
    "# test.to_csv('/Users/ivowings/Sync/Thesis/Datasources/Job vacancies/emscad_v1_anonymized.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['job_description'] = df['job_description'].progress_apply(name_remover)\n",
    "# df.to_csv(\"insert path\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
